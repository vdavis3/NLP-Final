{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7fPmq8e8bADk",
        "outputId": "9844608e-4e0b-40bc-d7be-9b5537259502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.36.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.6)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers",
                  "gast",
                  "google",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "787acee94cc44d68beb45099eb046a36"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0uaNjj4-YA2g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay9jAL3Lao3j",
        "outputId": "05565ac6-075a-4bbf-c9dd-74bbecaa5cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUGeRSGvaB-n"
      },
      "outputs": [],
      "source": [
        "prefix = 'translate biased to unbiased: '\n",
        "df2 = pd.read_csv('drive/MyDrive/biased.full', sep='\\t', names=[\"id\", \"src_tok\", \"tgt_tok\", \"src_raw\", \"tgt_raw\", \"src_POS_tags\", \"tgt_parse_tags\"], on_bad_lines='skip')\n",
        "df2['text_pairs_dict'] = df2.apply(lambda row: ({'orig': prefix + row['src_raw'].lower(), 'target': row['tgt_raw'].lower()}), axis=1)\n",
        "text_pairs_dict = df2.text_pairs_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icDSDMpAaXVd"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(text_pairs)\n",
        "num_valid_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_valid_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "valid_pairs = text_pairs[num_train_samples : num_train_samples + num_valid_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_valid_samples :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt3xbZL3bf2_"
      },
      "outputs": [],
      "source": [
        "train_file = 'drive/MyDrive/train_pairs.csv'\n",
        "valid_file = 'drive/MyDrive/valid_pairs.csv'\n",
        "test_file = 'drive/MyDrive/test_pairs.csv'\n",
        "\n",
        "pd.DataFrame(train_pairs).to_csv(train_file)\n",
        "pd.DataFrame(valid_pairs).to_csv(valid_file)\n",
        "pd.DataFrame(test_pairs).to_csv(test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVeQhl_XYFsp",
        "outputId": "75b6bfea-e0f5-40b1-cde6-0bd36bf89585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load files\n",
        "train_pairs = pd.read_csv('drive/MyDrive/train_pairs.csv')\n",
        "valid_pairs = pd.read_csv('drive/MyDrive/valid_pairs.csv')\n",
        "test_pairs = pd.read_csv('drive/MyDrive/test_pairs.csv')\n",
        "\n",
        "train_pairs = train_pairs.text_pairs_dict\n",
        "valid_pairs = valid_pairs.text_pairs_dict\n",
        "test_pairs = test_pairs.text_pairs_dict\n",
        "\n",
        "train_file = 'drive/MyDrive/train_pairs.csv'\n",
        "valid_file = 'drive/MyDrive/valid_pairs.csv'\n",
        "test_file = 'drive/MyDrive/test_pairs.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x6rjZotVYN8b"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(text_pairs, tokenizer, model, max_length=128):\n",
        "    orig_text = [orig for orig, target in text_pairs]\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        orig_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    orig_input_ids = np.array(orig_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    orig_attention_masks = np.array(orig_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    target_text = [target for orig, target in text_pairs]\n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        target_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(target_encoded['input_ids'])\n",
        "    decoder_input_ids = model._shift_right(label_ids)\n",
        "\n",
        "    return [orig_input_ids, orig_attention_masks, decoder_input_ids], label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5cAKh7hxYJCn"
      },
      "outputs": [],
      "source": [
        "class TranslationDataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        return self.n_examples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "        text_pairs = df.text_pairs_dict.apply(lambda x: ast.literal_eval(x).values())\n",
        "        #text_pairs = df[['orig', 'target']].values.astype(str).tolist()\n",
        "\n",
        "        batch_data = preprocess_data(\n",
        "            text_pairs,\n",
        "            self.tokenizer,\n",
        "            self.model,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ZRoMX4YXED",
        "outputId": "a0d8b829-49bf-457b-872b-9b706ce422f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained tensorflow model\n",
        "\n",
        "model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = TFT5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S0h8MPPYYYYm"
      },
      "outputs": [],
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 128\n",
        "batch_size = 64\n",
        "\n",
        "train_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4k4f_MzqYapP"
      },
      "outputs": [],
      "source": [
        "def build_t5_training_wrapper_model(t5_model, max_length):\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='labels')\n",
        "\n",
        "    t5_logits = t5_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[t5_logits])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hsv20hMfYctW"
      },
      "outputs": [],
      "source": [
        "model_wrapper = build_t5_training_wrapper_model(t5_model, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0KuL8TtlYfFz"
      },
      "outputs": [],
      "source": [
        "# As in the first notebook, we should add a model checkpoint callback to save\n",
        "# the trained model weights after each epoch. Edit the filepath to where\n",
        "# you want to save the weights in your own Drive\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/'\n",
        "checkpoint_filepath = checkpoint_dir + 't5_bias_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKComk1QYgXH",
        "outputId": "e667b541-8e0e-443b-9a1e-e5d10554d548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1984/1984 [==============================] - 53047s 27s/step - loss: 0.1366 - accuracy: 0.9761 - val_loss: 0.1158 - val_accuracy: 0.9789\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7be9a01bf3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Now call .fit on the model_wrapper, passing in the data generators and the\n",
        "# model checkpoint callback\n",
        "\n",
        "model_wrapper.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=1,\n",
        "                  callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDefs6ZnYiKx",
        "outputId": "f6b01c4b-e8eb-4670-c87e-8427cb33a13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lead programmers usually spend their careers mired in obscurity.']\n",
            "[\"the lyrics are about humanity's perceived idea of hell.\"]\n",
            "['marriage is a union of individuals.']\n"
          ]
        }
      ],
      "source": [
        "prefix = 'translate biased to unbiased: '\n",
        "\n",
        "for test_input_text in ['A lead programmer usually spends his career mired in obscurity.',\n",
        "                        \"The lyrics are about mankind's perceived idea of hell.\",\n",
        "                        'Marriage is a holy union of individuals.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = t5_model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'translate biased to unbiased: '\n",
        "\n",
        "for test_input_text in ['A lead programmer usually spends his career mired in obscurity.',\n",
        "                        \"The lyrics are about mankind's perceived idea of hell.\",\n",
        "                        'Marriage is a holy union of individuals.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = t5_model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hFpYUS9yPRS",
        "outputId": "065be892-1832-4a45-b80b-cfdfd0d150a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a lead programmer usually spends his career in obscurity.']\n",
            "[\"the lyrics are about humankind's perceived idea of hell.\"]\n",
            "['marriage is a union of individuals.']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}